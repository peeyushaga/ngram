{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LsoW3JucUvXq",
        "Ia6_JtNrUsBO"
      ],
      "mount_file_id": "1ggvd88W23_4L1LcJeNh6eiChhgvtVWBU",
      "authorship_tag": "ABX9TyNLpS4d2nrYMZLTOyWuyvMw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushaga/ngram/blob/main/ngram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/ngram files/Donald-Tweets!.csv')"
      ],
      "metadata": {
        "id": "IDVwX9IMvSe4"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_text = data['Tweet_Text']\n",
        "\n",
        "with open(\"/content/drive/MyDrive/ngram files/Donald-Tweets!.txt\", \"w\") as output:\n",
        "    output.write(str(tweet_text))"
      ],
      "metadata": {
        "id": "_opFU0Rlvk2k"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "files = glob.glob('/content/drive/MyDrive/ngram files/archive/*.txt')\n",
        "len(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE8bnNLn8u4E",
        "outputId": "dc3dcf57-d747-4bd1-f63a-efe5b6b41c61"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "for file in files:\n",
        "    with open(file, 'r') as file:\n",
        "        file_content = file.read().replace('\\\\', '')  # Replace backslashes with an empty string\n",
        "        text.append(file_content.replace('\\\\', ''))\n"
      ],
      "metadata": {
        "id": "ubJfc9M1ERSZ"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string  # Import the string module to get a list of punctuation characters\n",
        "\n",
        "# Define a translation table to remove punctuations\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "text = []\n",
        "for file in files:\n",
        "    with open(file, 'r') as file:\n",
        "        file_content = file.read().replace('\\\\', '')  # Replace backslashes with an empty string\n",
        "        # Remove punctuations from the text\n",
        "        cleaned_content = file_content.translate(translator)\n",
        "        text.append(cleaned_content)"
      ],
      "metadata": {
        "id": "BbWmtlo2asvv"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "LsoW3JucUvXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "lx_ZAJVDPBrJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "load train.txt and test.txt\n",
        "(NOTE: both must be stored in the same directory)\n",
        "and return train and test sets, as list of senetences\n",
        "'''\n",
        "def load_data(data_dir):\n",
        "\n",
        "  train_path = data_dir + 'train.txt'\n",
        "  test_path  = data_dir + 'test.txt'\n",
        "  with open(train_path,'r') as f:\n",
        "    train = [l.strip() for l in f.readlines()]\n",
        "  with open(test_path,'r') as f:\n",
        "    test = [l.strip() for l in f.readlines()]\n",
        "\n",
        "  return train, test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = load_data('/content/drive/MyDrive/ngram files/')"
      ],
      "metadata": {
        "id": "Du5ILp0mTtV3"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbiIiE8SUUC5",
        "outputId": "d734a0d3-77de-42ca-fa7c-634c1a389448"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['liberty all star usa sets initial payout',\n",
              " 'we are being accused of not implementing this agreement',\n",
              " 'entregrowth closed at 135 dlrs and options at 55 cents',\n",
              " 'usda forecast south african 1986 87 corn exports at 210 mln tonnes vs 300 mln tonnes last month and 1985 86 exports at 275 mln tonnes vs 275 mln tonnes last month',\n",
              " 'norgolds issued capital will be 2405 mln shares of which 63 pct will be held by nbh after 89 mln are issued to shareholders to raise 196 mln dlrs it said',\n",
              " 'the april 6 sale to be evenly divided between the three and six month issues will result in a paydown of 165 billion dlrs as maturing bills total 1485 billion dlrs',\n",
              " 'waste managements tender offer announced before the opening today expires march 25',\n",
              " 'he earlier estimated the damage from the us raid at about 500 mln dlrs',\n",
              " 'brougher bigi to sell 40 pct of subsidiary',\n",
              " 'that was not the case two years ago']"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgYISUY5UVlQ",
        "outputId": "8769658f-9c3e-4b46-b531-d1ea9e4c33f1"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(len(sentence) for sentence in train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se8IH74JUbFO",
        "outputId": "ed51435e-6b44-4d2a-9ca0-8c3cc4c33777"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3453"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Ia6_JtNrUsBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "Wl8jD7kjUhx9"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS = \"<s> \"\n",
        "EOS = \" </s>\"\n",
        "UNK = \"<UNK>\""
      ],
      "metadata": {
        "id": "wEacn7CSU1bK"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_sentence_tokens(sentences, n):\n",
        "  '''\n",
        "  add SOS (start of sentence) and EOS (end of sentence) tokens to each sentence\n",
        "  n is the order of n-gram model being used\n",
        "  add (n-1) SOS for n>=2, 1 SOS otherwise\n",
        "  returns a list of sentences wrapped with SOS and EOS tokens\n",
        "  '''\n",
        "  sos = SOS * (n-1) if n>=2 else SOS\n",
        "  return [f'{sos}{s.lower()}{EOS}' for s in sentences]"
      ],
      "metadata": {
        "id": "gRrf3JvbU_Sv"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(sentences):\n",
        "  '''\n",
        "  splits each sentence into tokens\n",
        "  1 token = 1 word or SOS or EOS or UNK\n",
        "  '''\n",
        "  return ' '.join(sentences).split(' ')"
      ],
      "metadata": {
        "id": "ABOt93kzWmrG"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_singletons(tokens):\n",
        "  '''\n",
        "  replaces all words,in the tokens list, that occur only once with UNK token\n",
        "  '''\n",
        "  vocab = nltk.FreqDist(tokens)\n",
        "  return [token if vocab[token] > 1 else UNK for token in tokens]"
      ],
      "metadata": {
        "id": "N7hTkvCqXabc"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_singletons_without_library(tokens):\n",
        "  '''\n",
        "  replaces all words,in the tokens list, that occur only once with UNK token without using nltk library\n",
        "  '''\n",
        "  vocab = {}\n",
        "  for token in tokens:\n",
        "    vocab[token] = vocab.get(token,0)+1\n",
        "  return [token if vocab[token]>1 else UNK for token in tokens]"
      ],
      "metadata": {
        "id": "uBGI8G-UYN26"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentences, n):\n",
        "  '''\n",
        "  entire preprocessing pipeline\n",
        "  adds SOS, EOS and UNK tokens to sentences and tokenizes\n",
        "  return preprocessed sentences tokenized by words\n",
        "  '''\n",
        "  sentences = add_sentence_tokens(sentences, n)\n",
        "  tokens = get_tokens(sentences)\n",
        "  tokens = replace_singletons(tokens)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "wbKrjUlXZQmU"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating N-gram"
      ],
      "metadata": {
        "id": "mrk6BqeObobE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = preprocess(train, 4)"
      ],
      "metadata": {
        "id": "r_KCXc1smpe7"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(tokens, n=3, laplace = 1):\n",
        "  '''\n",
        "  create a probability distribution (Laplace smoothed relative frequencies) of the training corpus\n",
        "  returns a mapping of each ngram with its probabilty\n",
        "\n",
        "  NOTE: m here refers to n-1\n",
        "  '''\n",
        "  vocab_size = len(nltk.FreqDist(tokens))\n",
        "\n",
        "  n_grams = nltk.ngrams(tokens, n)\n",
        "  n_vocab = nltk.FreqDist(n_grams)\n",
        "\n",
        "  m_grams = nltk.ngrams(tokens, n-1)\n",
        "  m_vocab = nltk.FreqDist(m_grams)\n",
        "\n",
        "  def smoothed_count(n_gram, n_count):\n",
        "    m_gram = n_gram[:-1]\n",
        "    m_count = m_vocab[m_gram]\n",
        "\n",
        "    # Probabilty of the nth word given (n-1) words\n",
        "    # P[n|m] = P[n^m]/P[m]\n",
        "    # therefore, P[ngram]/P[mgram]\n",
        "    return (n_count + laplace)/(m_count + laplace * vocab_size)\n",
        "\n",
        "  return {n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items()}\n"
      ],
      "metadata": {
        "id": "bq3WjEgWRjFu"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(tokens,n=4)"
      ],
      "metadata": {
        "id": "lPOVwSTWU88K"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def best_candidate(model, prev, i, without=[]):\n",
        "\n",
        "  blacklist = ['<UNK>']+without\n",
        "  candidates = ((ngram[-1],prob) for ngram, prob in model.items() if ngram[:-1]==prev)\n",
        "  candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
        "  candidates = sorted(candidates, key = lambda candidate: candidate[1], reverse = True)\n",
        "\n",
        "  if len(candidates)==0:\n",
        "    return (\" </s>\",1)\n",
        "  else:\n",
        "    return candidates[0 if prev!=() and prev[-1] != '<s>' else i]"
      ],
      "metadata": {
        "id": "oJxTW_wtVSZ2"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "xgmyIeAmo0ZN"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentences(model, num=10, min_len=12, max_len=30,n=3):\n",
        "  for i in range(num):\n",
        "    sent, prob = [\"<s>\"] * max(1, n-1), 1\n",
        "    while sent[-1] != \"</s>\":\n",
        "      prev = () if n == 1 else tuple(sent[-(n-1):])\n",
        "      blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
        "      next_token, next_prob = best_candidate(model, prev, i, without=blacklist)\n",
        "      sent.append(next_token)\n",
        "      prob *= next_prob\n",
        "\n",
        "      if len(sent) >= max_len:\n",
        "        sent.append(\"</s>\")\n",
        "        break\n",
        "\n",
        "    yield ' '.join(sent), -1/math.log(prob)\n"
      ],
      "metadata": {
        "id": "xIVeV4Edocs6"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence, prob in generate_sentences(model, num=5, min_len = 12, max_len = 30,n=4):\n",
        "  print(\"{} ({:.5f})\".format(sentence, prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOtLTnb4pwnE",
        "outputId": "a203a74d-2c32-49a8-8c5f-54e49c43cf91"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> <s> <s> the company said it will offer a stake in burlington industries inc and gencorp  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s> </s> (0.01005)\n",
            "<s> <s> <s> it said the new agreement will at its option convert to a four pct annual rate in  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s> </s> (0.00731)\n",
            "<s> <s> <s> shr loss five cts vs profit three  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s> </s> (0.02269)\n",
            "<s> <s> <s> he said the company has not yet been determined it will release an announcement this weekend that  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s> </s> (0.00749)\n",
            "<s> <s> <s> in a filing with the securities and exchange commission it has acquired an eight pct coupon to yield 810  </s>  </s>  </s>  </s>  </s>  </s>  </s>  </s> </s> (0.00784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUWlxsBdqRU8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}